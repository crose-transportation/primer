{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A More General Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings  # Ignore deprecation warnings in sklearn (clutters results)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Bring in all the models we're willing to consider:\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using classes, we can abstract out much of the ugliness we touched on in our iris example.\n",
    "# While it looks ugly compared to the imperative code we used earlier, there is no additional\n",
    "# functionality here; this is purely an exercise in iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Selector():\n",
    "        \n",
    "    '''Non-operational bag of Methods for selecting between model types and feature inputs given a model.  Parent of the\n",
    "    operant classes GenericClassifier and GenericRegressor'''    \n",
    "    \n",
    "    def ModelSelection(self, folds=10, rstate=420):\n",
    "        \n",
    "        cv_scores, cv_summary = {}, {}\n",
    "        \n",
    "        \n",
    "        for name, model in self.Models.items():\n",
    "            \n",
    "            try:\n",
    "            \n",
    "                kfold = model_selection.KFold(n_splits=folds, random_state=rstate) \n",
    "                cv_result = model_selection.cross_val_score(model, self.X, self.y, cv=kfold, scoring='accuracy')\n",
    "                cv_summary = \"%s: %f (%f)\" % (name, cv_result.mean(), cv_result.std())\n",
    "                cv_scores[name] = cv_result       \n",
    "                \n",
    "            \n",
    "            except Exception as e:\n",
    "                \n",
    "                cv_scores[name] = e\n",
    "                cv_summary[name] = e\n",
    "        \n",
    "        self.cv_scores = cv_scores\n",
    "        \n",
    "        # Print Summary\n",
    "        print('Model |', 'MSE |', 'Standard Deviation', '\\n')\n",
    "        for k, v in self.cv_scores.items():\n",
    "    \n",
    "            msg = \"%s: %f (%f)\" % (k, v.mean(), v.std())\n",
    "            print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericClassifier(Selector):\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        self.Models = {\n",
    "                       \n",
    "            'LR': LogisticRegression(),\n",
    "            'KNN': KNeighborsClassifier(),\n",
    "            'GBT': GradientBoostingClassifier(),\n",
    "            'NB': GaussianNB(),\n",
    "            'SVM': SVC(),\n",
    "            'DT': DecisionTreeClassifier()\n",
    "        \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the iris data\n",
    "iris = load_iris()\n",
    "\n",
    "# Create X (features) and Y (targets)\n",
    "X = iris.data\n",
    "Y, y = iris.target, iris.target\n",
    "\n",
    "Classifier = GenericClassifier(X, y) # Create an abstract classifier that holds several algorithm types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model | MSE | Standard Deviation \n",
      "\n",
      "LR: 0.880000 (0.148474)\n",
      "KNN: 0.933333 (0.084327)\n",
      "GBT: 0.926667 (0.096379)\n",
      "NB: 0.946667 (0.058119)\n",
      "SVM: 0.953333 (0.052068)\n",
      "DT: 0.953333 (0.052068)\n"
     ]
    }
   ],
   "source": [
    "Classifier.ModelSelection() # Show me the results of a standard cross validation on all discrete models I'm considering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's it; all we have to do is spin up a classifier of the instance that reported highest accuracy (SVM in my case),\n",
    "# retrain it on all our data (if appropriate) and we're done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could even write our helper classes to a library of their own, say 'HubDataTools', so that future users would \n",
    "# get all of the functionality listed above by simply entering: \n",
    "\n",
    "    # import HubDataTools as hub\n",
    "    \n",
    "    # classifier = hub.GenericClassifier()\n",
    "    # classifier.fit(X, y)\n",
    "    # etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In principle, we can apply these methods directly to Hub problems and see if the out of the box solutions\n",
    "# are superior to more manual methods, or do a longer term business analysis with data transformations and formal priors\n",
    "# to get an intellectual defensible model.   \n",
    "\n",
    "# A handful of off-the-cuff ideas for grins:\n",
    "\n",
    "# Predicting the liklihood of types of OTM exceptions for orders before an order is released\n",
    "# Regression analysis to infer maximum willingness to pay for specific services\n",
    "# Assigning value to contracts based on historic profitability and service level data\n",
    "# etc.\n",
    "\n",
    "# There's *alot* more to model(and feature selection) than what's here, and I'm by no means an expert, but I think\n",
    "# this is the general idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

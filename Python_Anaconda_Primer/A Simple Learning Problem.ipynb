{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn - Machine Learning and Statistical Inference\n",
    "\n",
    "### Documentation - https://scikit-learn.org/stable/documentation.html\n",
    "\n",
    "### A Common Learning Algorithm - K Nearest Neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn has a few specific conventions with how they represent model building:\n",
    "\n",
    "# Classifiers are abstract objects that hold different attributes(model skeletons) and methods(the\n",
    "# algorithms that map data (independent variables) and target values (dependent variables) to the skeleton.  \n",
    "\n",
    "# For example: The LinearRegression Classifier might have the attribute 'coefficients' that holds estimated parameter\n",
    "# values after running the algorithm 'Ordinary Least Squares' on given data/targets.\n",
    "\n",
    "# You can spin up a new classifier by calling the appropriate method from the sklearn sub-library; sklearn organizes\n",
    "# itself by the kinds of problems that one might want to solve(sklearn.neighbors is for discrete dependent variable\n",
    "# sets, while sklearn.linear_model is used for continuous sets).  There's a good graphic of\n",
    "# the kinds of problems different algorithms attempt to solve here:\n",
    "\n",
    "# https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "\n",
    "# All classifiers in sklearn have 'fit' and 'predict' methods: 'fit' loads data onto a classifier, \n",
    "# while 'predict' accepts a new feature set (with the same kinds of information present in X during 'fit')\n",
    "# and returns a guess at the label (dependent variable value) that should belong to those features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Nearest Neighbors, or KNN, takes a look at a set of features (independent variable values) and selects the 'K'\n",
    "# training observations that the features are most similar(shortest geometric distance) to.  Then, KNN assigns a \n",
    "# label to the feature set based on the labels on those 'K' neighbors.  In the simplest case(K=1), KNN assigns a label\n",
    "# to incoming data based on the label of the training data with the shortest euclidian distance between itself \n",
    "# and the data to be classified.  Luckily, sklearn has done all of the tedious work of actually writing the\n",
    "# KNN algorithm, so we can simply plagiarize their work!\n",
    "\n",
    "# Lets apply KNN to classify iris flowers using the familiar dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris # alternative source for the iris dataset (with data attribute as a numpy arrays) \n",
    "\n",
    "iris = load_iris() # load the data and store under 'iris'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data[0:5] # display the first couple values from our independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target[0:5] # display the first couple values from our dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'setosa', 1: 'versicolor', 2: 'virginica'}\n"
     ]
    }
   ],
   "source": [
    "# Maps the iris names (human readable) to data values (model usable):\n",
    "\n",
    "names = {label: name for name, label in zip(iris.target_names, range(0, 3))} \n",
    "\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier # Bring in the sklearn classifier for KNN \n",
    "\n",
    "KNN = KNeighborsClassifier() # initialze an empty KNN classifier (a place to hold data and an algorithm as a function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By convention, we refer to independent and dependent variables as 'X' and 'y' respectively .  Lets\n",
    "# origanize our data to reflect this feature:\n",
    "\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As model builders, we want to avoid the 'over generalization' problem, where the data used to create model\n",
    "# parameters describes the data we've given it, but performs poorly when predicting outcomes outside\n",
    "# of the training set.  In theory, these issues can be caused by idiosyncracies in the data (not randomly sampled, low\n",
    "# observation counts, etc.), or by structural changes in the data generating process between the training and\n",
    "# prediction periods (financial models built on pre-2008 crash data might predict very poorly in 2009). Since we\n",
    "# operate in a world with ever-changing economic conditions, we should be especially sensitive to this problem.\n",
    "\n",
    "# To help us prevent against over-generalization, we reserve a subset of our data(a test set) that will not\n",
    "# be used in the training process.  If the model performs 'well' on predicted labels in the test set (which\n",
    "# we know the right answers to) then we can be more confident that our model will predict well on information\n",
    "# when it really counts (production).\n",
    "\n",
    "# Setting the random state variable to a single number simply ensures that the training/test split \n",
    "# will be the same each time we run the split procedure (so we can replicate our results).\n",
    "\n",
    "from sklearn.model_selection import train_test_split # Bring in the sklearn function for splitting data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 420) # Shuffle our data and break into quarters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN.fit(X_train, y_train) # Assign the training data to our classifier and run the nearest neighbor algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['versicolor', 'virginica', 'virginica', 'setosa', 'setosa', 'versicolor', 'setosa', 'versicolor', 'virginica', 'versicolor', 'virginica', 'versicolor', 'setosa', 'setosa', 'virginica', 'versicolor', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'virginica', 'versicolor', 'setosa', 'versicolor', 'versicolor', 'setosa', 'setosa', 'versicolor', 'versicolor', 'versicolor', 'virginica', 'virginica', 'setosa', 'setosa', 'versicolor', 'virginica', 'versicolor']\n"
     ]
    }
   ],
   "source": [
    "predictions = KNN.predict(X_test) # Use the model built from training data to predict labels for the reserved \n",
    "english_predictions = [names[p] for p in predictions] # Translate numeric lables back into english\n",
    "\n",
    "print(english_predictions) # Display the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# How did our model do?  Lets take a look at the the predicted labels of our test set, versus the actual labels:\n",
    "\n",
    "print(['Incorrect' for pred, actual in zip(predictions, y_test) if pred != actual]) # Print 'Incorrect' for every wrong guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So our model had 100% accuracy guessing labels in our test set.  Sklearn also has its own implementation for\n",
    "# gauging accuracy:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, predictions) # Returns correct guesses / total guesses as a floating point number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have a working classifier that boasts a perfect accuracy score, but who said that K Nearest Neighbors\n",
    "# would yield the 'best' solution?  How do I pick between alternative classifiers?\n",
    "\n",
    "# The standard ML answer to this question is to use cross validation.  Cross validation randomnly splits our data into\n",
    "# 'K' sections(you might see this referred to as 'K fold' validation, too).  If K = 5, our data is partitioned into 5\n",
    "# parts and 5 separate models are trained using 4 sections of the data to train and 1 rotating section of test data.\n",
    "# Then, accuracy scores for each are computed (typically in terms of mean squared error, but you can pick any penalty\n",
    "# function you want) and compared across each of the models.  The use of multiple folds tells us wether or not the model we're\n",
    "# using is sensitive to how the data is sampled; we prefer higher accuracy scores and lower variance between those\n",
    "# scores, all else the same.\n",
    "\n",
    "# A note on cross validation: This is analogous to making modeling decisions based on nothing but R^2 in the context\n",
    "# of traditional stat modeling, which can be dangerous.  There's a big spiel about this issue that would be better handled\n",
    "# in another context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98039216, 0.98039216, 1.        ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets compute the cross validation score for a K Nearest Neighbor Model\n",
    "\n",
    "import sklearn.model_selection as model_selection\n",
    "\n",
    "# Note: If we are using cross validation, the train/test split procedure is handled automatically, so\n",
    "# we can pass the original data instead of the splits\n",
    "\n",
    "# The following makes 3 KNN models with 3 unique splits and scores the accuracy of each:\n",
    "model_selection.cross_val_score(KNN, X, y, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98039216, 0.92156863, 0.97916667])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can run the same procedure on an alternative classifier and make a decision based on the scores\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier # A competing classifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "model_selection.cross_val_score(tree, X, y, cv=3, scoring='accuracy') # Perform the same procedure using a tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our results may differ due to random shuffling, but my results show a higher average accuracy score and\n",
    "# reduced variance for the KNN model, which suggests KNN is superior to the tree model here.\n",
    "\n",
    "# Now that we are somewhat happy with the average accuracy of the KNN process on this problem and we did not\n",
    "# observe a high variance in accuracy score with respect to sampling, we can retrain the KNN classifier\n",
    "# on all of the available data:\n",
    "\n",
    "KNN = DecisionTreeClassifier() # initialize an empty shell, writing over our old KNN model\n",
    "Final_Model = KNN.fit(X, y) # Fit all of the avilable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we're done.  Our 'Final_Model' is ready to label new incoming feature sets with iris names.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a multitude of algorithms available to us, however.  How do we abstract this model selection problem\n",
    "# so that we aren't constantly writing and rewriting accuracy tests?  Take a look in the 'Generalized Machine Learning'\n",
    "# notebook for an attempt at solving this problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
